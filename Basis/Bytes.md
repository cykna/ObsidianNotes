When starting to work with computers, we must understand how data is structured.
The way data is structured is called bytes. It's a sequence of 8 bits. Where a bit is actually named as 'binary digit'. The value of a bit is either 0 or 1, on the physics, it's when a transistor is 'activated', if so, it's 1, if not is 0. The group of 8 bits is called a byte.
In programming, in reality everything is an abstraction to what a byte is. For example, [instructions](./Instructions.md) are simply bytes that a processor understands and based them knows what task to execute. [strings](../String/String.md) are in fact an [array](../DataStructures/Array.md) of bytes where each byte represents a char, or, depending of the [Encoding](../Strings/Encoding.md) of so, a group of bytes represent a char. Same thing for numbers. When working with [int](./Primitives.md) in C for example, the 'int' type is actually a '4 byte signed integer'.
A [pointer](./Pointer.md) is in 64 bit architecture, 8 bytes, and in 32 bit architecture 4 bytes, the same size as an usize. [Objects](../Concepts/OOP), supposing [OOP](../Concepts/OOP) are just pointers to the [Heap](../DataStructures/Heap) that actually contain the information about the object, which is a contiguous byte array that interprets the bytes in a order.
The primitives, in case, the types that are more close to bytes than any others are numbers. The reason is very simple actually: they're simple bytes that are interpreted a way we can understand.
When we print a number on the console for example, let's say we get '5', actually what happened is that the bits of the number are 101 and we converted it to it's [ascii](../Strings/Encodings/Ascii) equivalent. Simple as that.
In programming we have a lot of types of numbers. In languages like Javascript or Python or other high level ones we generally use a single type for everything. In rust for example we got i8, i16, i32, i64, i128 which are signed integers followed by the amount of bits each use. The same logic for 'u' variants: u8, u16, u32, u64, u128. Rust follows IEEE-752 for implementing floats, or decimal values, and it contains f32 and f64, equivalents to 'float' and 'double' in C. Actually, the only type that Javascript supports is f64 if not using typed arrays.
Everything is a set of bytes. A byte can represent up to 2‚Å∏ values. 2 because a Bit is able to be only 1 or 0, thus 2 states, and power of 8 because we got a group of 8 bits. Then on inserting a new bit we double the amount of possible values. If 9 bits, it will now be able to handle 512 values. But by convention, the types will always use an amount of bits that is a power of 2, as mentioned above of i8,i16,i32,i64 and i128. If we need more values, we will then make an i256, i512, i1024 and so on, but that might not happen soon. In fact on manipulating bytes, explained a bit on [primitives](./Primitives.md), we can store any amount of data we want, 5 bytes, 3 bytes, 7 bytes, 21 bits, 19 bits, 13 bits, etc. The thing is that it's simply a convention.